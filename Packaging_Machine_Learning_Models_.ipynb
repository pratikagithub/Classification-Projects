{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNriZD9pfqHZ8n4IXQVsBX9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikagithub/Classification-Projects/blob/main/Packaging_Machine_Learning_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model packaging is an essential step in the Machine Learning deployment process, where the trained model is prepared in a format that can be easily deployed and integrated into production environments.\n",
        "\n",
        "**Packaging Machine Learning Models with Python**\n",
        "\n",
        "Packaging Machine Learning models involves saving it and wrapping it with an API. Here’s the process of packaging a Machine Learning model:\n",
        "\n",
        "Step 1: Export/Save the Trained Model\n",
        "\n",
        "Step 2: Writing a Wrapper Function to Load the Model\n",
        "\n",
        "Step 3: Setting Up an API to Serve the Model\n",
        "\n",
        "Let’s go through all these steps one by one."
      ],
      "metadata": {
        "id": "wfYi-E_9zysl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Export/Save the Trained Model**\n",
        "\n",
        "Once you have trained a model, the first step is to save it in a format that can be loaded during deployment. Different libraries have different ways of saving models.\n",
        "\n",
        "Let’s train an example model on the iris dataset and save it using the pickle method:"
      ],
      "metadata": {
        "id": "4sW3ALs80DMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QunAj8yzv5u",
        "outputId": "c6a9b251-28f1-491f-8bee-95d8786941fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
        "\n",
        "# train model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# save the model using joblib\n",
        "joblib.dump(model, 'rf_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Writing a Wrapper Function to Load the Model**\n",
        "\n",
        "For deployment, you’ll need a function that loads the model and makes predictions. This wrapper function will be exposed through an API or used in the production environment. Here’s how to write the wrapper function:"
      ],
      "metadata": {
        "id": "_8-Ttco60Mwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load the saved model\n",
        "def load_model():\n",
        "    model = joblib.load('rf_model.pkl')\n",
        "    return model\n",
        "\n",
        "# function to make predictions\n",
        "def predict(input_data):\n",
        "    model = load_model()\n",
        "    prediction = model.predict(input_data)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "ExSfsZwO0T3k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Setting Up an API to Serve the Model**\n",
        "\n",
        "To serve your model in production, you can expose it via an API. Flask or FastAPI is often used to create REST APIs for this purpose.\n",
        "\n",
        "Here’s how to set up an API to serve your model (please make sure to write this piece of code in a separate Python file):"
      ],
      "metadata": {
        "id": "amgvZWaq0WqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# load the model\n",
        "model = joblib.load('rf_model.pkl')\n",
        "\n",
        "# define a route for making predictions\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json()  # Get input data from POST request\n",
        "    input_data = np.array(data['input']).reshape(1, -1)  # Reshape input\n",
        "    prediction = model.predict(input_data)\n",
        "    return jsonify({'prediction': int(prediction[0])})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TthThMeo0cU0",
        "outputId": "89b0cb86-7be8-4f78-f39d-7fe988b36c7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run this file. Once you run this file, you can test the API using tools like Postman. Postman is a popular API development and testing tool that simplifies the process of sending requests to APIs and inspecting the responses. It is widely used by developers for testing, building, and documenting APIs.\n",
        "\n",
        "Please download the desktop version of Postman on your system. And follow these steps to test your API:\n",
        "\n",
        "1. Open Postman and create a new POST request.\n",
        "\n",
        "2. Set the URL to http://127.0.0.1:5000/predict (Replace the URL with your URL).\n",
        "\n",
        "3. In the Body tab, choose raw and set the data type to JSON.\n",
        "\n",
        "Now, enter the following example JSON data in the body (adjust according to your model’s input size):"
      ],
      "metadata": {
        "id": "ovwndEEg0jCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "\n",
        "    \"input\": [0.5, 0.3, 0.7, 0.1]\n",
        "    \n",
        "}\n",
        "\n",
        "// This is an example input for the Machine Learning model."
      ],
      "metadata": {
        "id": "kLqyp6Hq4SCm"
      }
    }
  ]
}